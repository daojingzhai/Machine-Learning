\documentclass[a4paper,UTF8]{article}
\usepackage{ctex}
\usepackage[margin=1.25in]{geometry}
\usepackage{color}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{bm}
\usepackage{hyperref}
\usepackage{epsfig}
\usepackage{color}
\usepackage{mdframed}
\usepackage{lipsum}
\usepackage{footnote}
\usepackage{times}
\usepackage{mathtools}
\newmdtheoremenv{thm-box}{myThm}
\newmdtheoremenv{prop-box}{Proposition}
\newmdtheoremenv{def-box}{定义}
\usepackage{fontspec}
\usepackage{float}

\setlength{\evensidemargin}{.25in}
\setlength{\textwidth}{6in}
\setlength{\topmargin}{-0.5in}
\setlength{\topmargin}{-0.5in}
% \setlength{\textheight}{9.5in}
%%%%%%%%%%%%%%%%%%此处用于设置页眉页脚%%%%%%%%%%%%%%%%%%
\usepackage{fancyhdr}                                
\usepackage{lastpage}                                           
\usepackage{layout}                                             
\footskip = 10pt 
\pagestyle{fancy}                    % 设置页眉                 
\lhead{2018年春季}                    
\chead{数据挖掘导论}                                                
% \rhead{第\thepage/\pageref{LastPage}页} 
\rhead{作业三}                                                                                               
\cfoot{\thepage}                                                
\renewcommand{\headrulewidth}{1pt}  			%页眉线宽，设为0可以去页眉线
\setlength{\skip\footins}{0.5cm}    			%脚注与正文的距离           
\renewcommand{\footrulewidth}{0pt}  			%页脚线宽，设为0可以去页脚线

\makeatletter 									%设置双线页眉                                        
\def\headrule{{\if@fancyplain\let\headrulewidth\plainheadrulewidth\fi%
\hrule\@height 1.0pt \@width\headwidth\vskip1pt	%上面线为1pt粗  
\hrule\@height 0.5pt\@width\headwidth  			%下面0.5pt粗            
\vskip-2\headrulewidth\vskip-1pt}      			%两条线的距离1pt        
 \vspace{6mm}}     								%双线与下面正文之间的垂直间距              
\makeatother  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\numberwithin{equation}{section}
%\usepackage[thmmarks, amsmath, thref]{ntheorem}
\newtheorem{myThm}{myThm}
\newtheorem*{myDef}{Definition}
\newtheorem*{mySol}{Solution}
\newtheorem*{myProof}{Proof}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}

\usepackage{multirow}

%--

%--
\begin{document}
\title{数据挖掘导论\\
作业三}
\author{151250189, 翟道京, zhaidj@smail.nju.edu.cn}
\maketitle

\begin{abstract}
\large{在本次数据挖掘任务中，我们使用Weka软件，对所提供的十组数据集进行多种分类器的训练，并集中比较了六种学习方式的优异性；文末我们对基于CNN方法的Bagging集成学习进行了性能优化，通过相关参数修改，提升了对german\_credit数据集的分类功能。}
\end{abstract}

\section{任务描述}
Try J4.8 (C4.5), Naïve Bayes, SVM, Neural Network, kNN, and their ensemble version using Bagging on provided data sets based on 10-fold cross validation, can compare their performances w.r.t, accuracy, and AUC. Discuss on their performance and suggest how to improve Bagging of KNN (with necessary experimental evidence).

\section{数据描述}
本次数据挖掘实验采用十组数据集如下所列
\begin{itemize} 
\item wisconsin-breast-cancer
\item horse-colic
\item credit-rating
\item german\_credit
\item pima\_diabetes
\item hepatitis
\item mozilla4
\item PC1
\item pc5
\item waveform
\end{itemize}

十组数据集样本维度各不同，均为二分类任务。


\section{实验方法}
本次作业选用Weka软件实现J4.8 (C4.5), Naïve Bayes, SVM, Neural Network, kNN与Bagging功能，使用如下内建函数
\begin{itemize}
\item weka.classifiers.trees.J48
\item weka.classifiers.bayes.NaiveBayes
\item weka.classifiers.functions.SMO
\item weka.classifiers.functions.MultilayerPerceptron
\item weka.classifiers.lazy.IBK
\item weka.classifiers.meta.Bagging
\end{itemize}
各函数参数均选择初始默认参数设置。其中Bagging基于REPTree算法。使用10次交叉验证，通过Weka软件对相关训练器的训练，我们获得相应的学习性能指标\footnote{注意Weka显示的时间为模型构建时间，而非训练时间。}。

\section{实验结果}
\subsection{wisconsin-breast-cancer Data Set}
\begin{table}[H]
\centering
\caption{wisconsin-breast-cancer数据集训练结果}
\label{my-label}
\begin{tabular}{l|lllll}
\hline
Method         & Accuracy & Recall & ROC Area & PRC Area & Time/s \\ \hline
J4.8(C4.5)     & 0.946    & 0.946  & 0.955    & 0.937    & 0.06   \\ \hline
Naïve Bayes   & 0.962    & 0.960  & 0.986    & 0.976    & 0.02   \\ \hline
SVM(SMO)       & 0.970    & 0.970  & 0.968    & 0.957    & 0.05   \\ \hline
Neural Network & 0.953    & 0.953  & 0.896    & 0.984    & 0.64   \\ \hline
1-NN           & 0.954    & 0.954  & 0.988    & 0.986    & 0.00   \\ \hline
Bagging        & 0.965    & 0.964  & 0.990    & 0.987    & 0.16   \\ \hline
\end{tabular}
\end{table}

\subsection{horse-colic Data Set}

\begin{table}[H]
\centering
\caption{horse-colic数据集训练结果}
\label{my-label}
\begin{tabular}{l|lllll}
\hline
Method         & Accuracy & Recall & ROC Area & PRC Area & Time/s \\ \hline
J4.8(C4.5)     & 0.854    & 0.853  & 0.813    & 0.788    & 0.02   \\ \hline
Naïve Bayes   & 0.788    & 0.780  & 0.842    & 0.843    & 0.01   \\ \hline
SVM(SMO)       & 0.825    & 0.826  & 0.809    & 0.769    & 0.17   \\ \hline
Neural Network & 0.806    & 0.804  & 0.857    & 0.848    & 4.67   \\ \hline
1-NN           & 0.811    & 0.813  & 0.802    & 0.766    & 0.00   \\ \hline
Bagging        & 0.864    & 0.864  & 0.893    & 0.892    & 0.16   \\ \hline
\end{tabular}
\end{table}

\subsection{credit-rating Data Set}

\begin{table}[H]
\centering
\caption{credit-rating数据集训练结果}
\label{my-label}
\begin{tabular}{l|lllll}
\hline
Method         & Accuracy & Recall & ROC Area & PRC Area & Time/s \\ \hline
J4.8(C4.5)     & 0.861    & 0.861  & 0.887    & 0.849    & 0.01   \\ \hline
Naïve Bayes   & 0.793    & 0.777  & 0.896    & 0.886    & 0.00   \\ \hline
SVM(SMO)       & 0.861    & 0.849  & 0.856    & 0.806    & 0.25   \\ \hline
Neural Network & 0.836    & 0.836  & 0.895    & 0.885    & 4.52   \\ \hline
1-NN           & 0.811    & 0.812  & 0.808    & 0.753    & 0.00   \\ \hline
Bagging        & 0.860    & 0.857  & 0.919    & 0.908    & 0.04   \\ \hline
\end{tabular}
\end{table}

\subsection{german-credit Data Set}
\begin{table}[H]
\centering
\caption{german-credit数据集训练结果}
\label{my-label}
\begin{tabular}{l|lllll}
\hline
Method         & Accuracy & Recall & ROC Area & PRC Area & Time/s \\ \hline
J4.8(C4.5)     & 0.687    & 0.705  & 0.639    & 0.657    & 0.02   \\ \hline
Naïve Bayes   & 0.743    & 0.754  & 0.787    & 0.797    & 0.00   \\ \hline
SVM(SMO)       & 0.738    & 0.751  & 0.671    & 0.681    & 0.29   \\ \hline
Neural Network & 0.713    & 0.715  & 0.730    & 0.757    & 12.36  \\ \hline
1-NN           & 0.716    & 0.720  & 0.660    & 0.669    & 0.00   \\ \hline
Bagging        & 0.732    & 0.747  & 0.762    & 0.773    & 0.06   \\ \hline
\end{tabular}
\end{table}

\subsection{pima-diabetes Data Set}
\begin{table}[H]
\centering
\caption{pima-diabetes数据集训练结果}
\label{my-label}
\begin{tabular}{l|lllll}
\hline
Method         & Accuracy & Recall & ROC Area & PRC Area & Time/s \\ \hline
J4.8(C4.5)     & 0.735    & 0.738  & 0.751    & 0.727    & 0.01   \\ \hline
Naïve Bayes   & 0.759    & 0.763  & 0.819    & 0.815    & 0.00   \\ \hline
SVM(SMO)       & 0.769    & 0.773  & 0.720    & 0.698    & 0.03   \\ \hline
Neural Network & 0.750    & 0.754  & 0.793    & 0.786    & 0.50   \\ \hline
1-NN           & 0.696    & 0.702  & 0.650    & 0.640    & 0.00   \\ \hline
Bagging        & 0.752    & 0.758  & 0.812    & 0.808    & 0.03   \\ \hline
\end{tabular}
\end{table}

\subsection{hepatitis Data Set}
\begin{table}[H]
\centering
\caption{hepatitis数据集训练结果}
\label{my-label}
\begin{tabular}{l|lllll}
\hline
Method         & Accuracy & Recall & ROC Area & PRC Area & Time/s \\ \hline
J4.8(C4.5)     & 0.825    & 0.839  & 0.708    & 0.800    & 0.00   \\ \hline
Naïve Bayes   & 0.853    & 0.845  & 0.860    & 0.891    & 0.00   \\ \hline
SVM(SMO)       & 0.847    & 0.852  & 0.756    & 0.803    & 0.01   \\ \hline
Neural Network & 0.807    & 0.800  & 0.823    & 0.848    & 0.28   \\ \hline
1-NN           & 0.794    & 0.806  & 0.653    & 0.747    & 0.00   \\ \hline
Bagging        & 0.786    & 0.813  & 0.800    & 0.829    & 0.01   \\ \hline
\end{tabular}
\end{table}

\subsection{mozilla4 Data Set}
\begin{table}[H]
\centering
\caption{mozilla4数据集训练结果}
\label{my-label}
\begin{tabular}{l|lllll}
\hline
Method         & Accuracy & Recall & ROC Area & PRC Area & Time/s \\ \hline
J4.8(C4.5)     & 0.949    & 0.948  & 0.954    & 0.953    & 0.42   \\ \hline
Naïve Bayes   & 0.785    & 0.686  & 0.829    & 0.824    & 0.02   \\ \hline
SVM(SMO)       & 0.848    & 0.832  & 0.838    & 0.800    & 2.65   \\ \hline
Neural Network & 0.911    & 0.912  & 0.940    & 0.944    & 5.88   \\ \hline
1-NN           & 0.890    & 0.890  & 0.877    & 0.857    & 0.01   \\ \hline
Bagging        & 0.951    & 0.950  & 0.975    & 0.979    & 0.82   \\ \hline
\end{tabular}
\end{table}

\subsection{pc1 Data Set}
\begin{table}[H]
\centering
\caption{pc1数据集训练结果}
\label{my-label}
\begin{tabular}{l|lllll}
\hline
Method         & Accuracy & Recall & ROC Area & PRC Area & Time/s \\ \hline
J4.8(C4.5)     & 0.917    & 0.933  & 0.668    & 0.901    & 0.02   \\ \hline
Naïve Bayes   & 0.899    & 0.892  & 0.650    & 0.900    & 0.00   \\ \hline
SVM(SMO)       & 0.866    & 0.930  & 0.500    & 0.871    & 0.05   \\ \hline
Neural Network & 0.921    & 0.936  & 0.723    & 0.916    & 2.19   \\ \hline
1-NN           & 0.922    & 0.921  & 0.740    & 0.918    & 0.00   \\ \hline
Bagging        & 0.931    & 0.941  & 0.847    & 0.943    & 0.06   \\ \hline
\end{tabular}
\end{table}

\subsection{pc5 Data Set}
\begin{table}[H]
\centering
\caption{pc5数据集训练结果}
\label{my-label}
\begin{tabular}{l|lllll}
\hline
Method         & Accuracy & Recall & ROC Area & PRC Area & Time/s \\ \hline
J4.8(C4.5)     & 0.972    & 0.975  & 0.817    & 0.967    & 1.72   \\ \hline
Naïve Bayes   & 0.966    & 0.964  & 0.833    & 0.971    & 0.12   \\ \hline
SVM(SMO)       & 0.966    & 0.972  & 0.541    & 0.946    & 12.68  \\ \hline
Neural Network & 0.966    & 0.971  & 0.941    & 0.981    & 101.63 \\ \hline
1-NN           & 0.972    & 0.973  & 0.932    & 0.979    & 0.01   \\ \hline
Bagging        & 0.972    & 0.976  & 0.975    & 0.987    & 2.57   \\ \hline
\end{tabular}
\end{table}

\subsection{waveform Data Set}
\begin{table}[H]
\centering
\caption{waveform数据集训练结果}
\label{my-label}
\begin{tabular}{l|lllll}
\hline
Method         & Accuracy & Recall & ROC Area & PRC Area & Time/s \\ \hline
J4.8(C4.5)     & 0.751    & 0.751  & 0.830    & 0.672    & 0.33   \\ \hline
Naïve Bayes   & 0.835    & 0.800  & 0.956    & 0.919    & 0.04   \\ \hline
SVM(SMO)       & 0.867    & 0.867  & 0.932    & 0.817    & 0.28   \\ \hline
Neural Network & 0.836    & 0.836  & 0.963    & 0.929    & 34.03  \\ \hline
1-NN           & 0.736    & 0.736  & 0.802    & 0.630    & 0.00   \\ \hline
Bagging        & 0.815    & 0.815  & 0.951    & 0.903    & 1.04   \\ \hline
\end{tabular}
\end{table}

\section{结果分析}
根据上述数据集的结果分析，得到以下结论
\begin{enumerate}
\item 简单的Mutilayer Perceptron(CNN)在不同训练集上都有着不俗的表现，但同时缺陷为最大的时间开销。
\item 就时间开销而言，KNN作为典型的lazy learning方法，时间开销较小；同时Naïve Bayes方法也有这较优秀的时间开销。
\item 就分类性能而言，对于不同的问题，判别式模型与生成式模型有着不同的表现，对于具体问题，应当在对数据特征有着深刻理解后，选择相应的学习方法。
\item 基于REPTree的简单集成学习Bagging方法对不同的问题都有着不俗的表现，展现出集成学习较为强大的泛化能力。
\end{enumerate}

\section{算法优化}
我们上述实验中的集成学习是基于REPTree进行Bagging，其中对部分数据集效果并不出众，我们以german\_credit数据集为例，探究提升Bagging of KNN的性能的方法。
\subsection{KNN属性的探究}
通过调节K-NN属性K值，对分类器性能进行提升，下表展示了单层Bagging下不同K值的分类器性能

\begin{table}[H]
\centering
\caption{My caption}
\label{my-label}
\begin{tabular}{l|lllll}
\hline
K-NN  & Precision & Recall & ROC Area & PRC Area & Time/s \\ \hline
1-NN  & 0.713     & 0.721  & 0.694    & 0.722    & 0.01   \\ \hline
3-NN  & 0.715     & 0.732  & 0.721    & 0.754    & 0.01   \\ \hline
5-NN  & 0.716     & 0.735  & 0.743    & 0.776    & 0.01   \\ \hline
10-NN & 0.717     & 0.737  & 0.755    & 0.777    & 0.01   \\ \hline
\end{tabular}
\end{table}
由上表可知，通过对本数据集的研究，调节K值对分类器性能有一定的提升。


\subsection{Bagging迭代次数的研究}
我们选择10-NN分类器，在单层Bagging下调节不同迭代次数，探究分类器的性能变化

\begin{table}[H]
\centering
\caption{My caption}
\label{my-label}
\begin{tabular}{l|lllll}
\hline
Iteration & Precision & Recall & ROC Area & PRC Area & Time/s \\ \hline
10        & 0.717     & 0.737  & 0.755    & 0.777    & 0.01   \\ \hline
20        & 0.730     & 0.747  & 0.753    & 0.779    & 0.01   \\ \hline
50        & 0.724     & 0.742  & 0.753    & 0.780    & 0.03   \\ \hline
100       & 0.727     & 0.744  & 0.755    & 0.782    & 0.03   \\ \hline
\end{tabular}
\end{table}
如上表所示，在一定范围内调节Bagging迭代次数，可提高分类器性能，但迭代次数超过一定范围后，提升效果不再显著。

\subsection{Bagging层嵌套}
我们选择Bagging迭代次数为10次，在基于10-NN分类标准下实现Bagging层的嵌套，从而提升分类器的性能
\begin{table}[H]
\centering
\caption{My caption}
\label{my-label}
\begin{tabular}{l|lllll}
\hline
Layers & Precision & Recall & ROC Area & PRC Area & Time/s \\ \hline
1      & 0.717     & 0.737  & 0.755    & 0.777    & 0.01   \\ \hline
2      & 0.726     & 0.743  & 0.761    & 0.787    & 0.05   \\ \hline
3      & 0.728     & 0.744  & 0.762    & 0.788    & 0.29   \\ \hline
4      & 0.747     & 0.746  & 0.788    & 0.793    & 4.38   \\ \hline
5      & 0.776     & 0.767  & 0.800    & 0.813    & 106.40 \\ \hline
\end{tabular}
\end{table}
可以发现，尽管牺牲了时间开销，多层Bagging嵌套分类器性能得到了提升。

\section{总结}
在本次实验中，我们使用Weka程序演绎了经典的几种机器学习分类方法，对相关方法性能进行评估比较，并通过调参实验，深入探讨了KNN-Bagging集成学习方法。本次实验后我又以下心得体会：
\begin{itemize}
\item 具体问题具体分析，对于不同的数据，我们应该在了解相应数据特征、分布类型后选择对应的学习方法。
\item 不同的学习方法的时间开销/空间开销中差异较大，有着特定的适用问题，我们需要了然于胸。
\item 掌握必要的调参方式，在机器学习，特别是深度学习中有着重要意义。
\end{itemize}
\end{document}